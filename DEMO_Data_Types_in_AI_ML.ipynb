{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravipatil33/llama-stack/blob/example-notebook/DEMO_Data_Types_in_AI_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWabE0rHBoqt"
      },
      "source": [
        "\n",
        "**Specialized Data Formats for AI/ML**\n",
        "\n",
        "* Parquet\n",
        "* HDF5\n",
        "* TFRecord\n",
        "* Image Formats (JPG, PNG)\n",
        "\n",
        "Parquet: Columnar storage format optimized for big data analytics.\n",
        "\n",
        "HDF5: Hierarchical format for large datasets with support for complex structures.\n",
        "\n",
        "TFRecord: TensorFlow’s format for large-scale datasets, particularly in deep learning.\n",
        "\n",
        "Image Formats: Optimized storage for image data (e.g., JPG, PNG).\n",
        "\n",
        "Demo on PyTorch Native Format - To store images and print any image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO3LRipRBoqw"
      },
      "source": [
        "[1] **Parquet Format**\n",
        "\n",
        "Usage:\n",
        "- Efficient for big data applications, supports columnar storage, used in distributed systems like Apache Spark.\n",
        "\n",
        "Advantages:\n",
        "- High compression, faster column-wise operations, good for numeric data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VrpbkIsBoqx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Name': ['Alpha', 'Beta', 'Charlie', 'Delta'],\n",
        "    'Age': [25, 30, 35, 32],\n",
        "    'Skill Level': [85.5, 90.2, 88.7, 92]\n",
        "})\n",
        "\n",
        "# Save DataFrame to Parquet\n",
        "df.to_parquet('data.parquet')\n",
        "\n",
        "# Read the Parquet file\n",
        "df_parquet = pd.read_parquet('data.parquet')\n",
        "print(df_parquet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QboyFuzYL1C5"
      },
      "source": [
        "This is single example.\n",
        "\n",
        "However it can be used to store different kinds of data.\n",
        "\n",
        "- Weather\n",
        "- List of travellers in Titanic\n",
        "- Flight Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xXDjDmCL_H6"
      },
      "outputs": [],
      "source": [
        "# Connect Colab to Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9Vp_5VRNG3m"
      },
      "outputs": [],
      "source": [
        "# Import file to Drive\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# List Files\n",
        "os.listdir()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uto6rfiBNjlp"
      },
      "outputs": [],
      "source": [
        "# Read content of the files : Parquet\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "\n",
        "# Read the Parquet file\n",
        "#df_parquet = pd.read_parquet('MT cars.parquet')\n",
        "df_parquet = pd.read_parquet('Flights 1m.parquet')\n",
        "#df_parquet = pd.read_parquet('Weather.parquet')\n",
        "#df_parquet = pd.read_parquet('Titanic.parquet')\n",
        "\n",
        "# Print entire content of data file.\n",
        "#print(df_parquet)\n",
        "\n",
        "# Print specific row in parquet data file\n",
        "print(df_parquet.iloc[0])\n",
        "\n",
        "# Get details of parquet file\n",
        "#pfile = pq.read_table('Titanic.parquet')\n",
        "\n",
        "# Print Schema of the data file\n",
        "#print(\"Column names: {}\".format(pfile.column_names))\n",
        "#print(\"Schema: {}\".format(pfile.schema))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCAqAO6LBoqy"
      },
      "source": [
        "[2] **HDF5 Format**\n",
        "\n",
        "Usage:\n",
        "- Efficiently handles large datasets and complex hierarchies. - Used extensively in scientific computing and deep learning.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Supports storage of large multi-dimensional arrays and hierarchical data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AzJ2a-CBoqy"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Create an HDF5 file\n",
        "with h5py.File('data.h5', 'w') as f:\n",
        "    # Create a dataset\n",
        "    data = f.create_dataset('dataset_1', (100,), dtype='i')\n",
        "    data[...] = np.arange(100)  # Fill with data\n",
        "\n",
        "# Read the HDF5 file\n",
        "with h5py.File('data.h5', 'r') as f:\n",
        "    print(f['dataset_1'][:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch2DafBTmaC1"
      },
      "outputs": [],
      "source": [
        "# Create arrays using HDF5 Format\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.random.randn(10000)\n",
        "arr2 = np.random.randn(10000)\n",
        "\n",
        "with h5py.File('complex_read.hdf5', 'w') as f:\n",
        "    f.create_dataset('array_1', data=arr1)\n",
        "    f.create_dataset('array_2', data=arr2)\n",
        "\n",
        "with h5py.File('complex_read.hdf5', 'r') as f:\n",
        "    d1 = f['array_1']\n",
        "    d2 = f['array_2']\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for i in range(len(d1)):\n",
        "        if d1[i] > 0:\n",
        "            data.append(d2[i])\n",
        "\n",
        "\n",
        "print('The length of data with a for loop: {}'.format(len(data)))\n",
        "\n",
        "# Print entire array\n",
        "#print(data)\n",
        "\n",
        "# Print specific entry\n",
        "print(data[5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyqELXybBoqz"
      },
      "source": [
        "[3] **TFRecord Format**\n",
        "\n",
        "Usage:\n",
        "- TensorFlow's preferred format for efficient storage of large datasets.\n",
        "\n",
        "Advantages:\n",
        "- Supports sequential reading of data for large-scale deep learning tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_B5_zc4Boqz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a function to create an example\n",
        "def create_example():\n",
        "    feature = {\n",
        "        'name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b'Ravindra'])),\n",
        "        'age': tf.train.Feature(int64_list=tf.train.Int64List(value=[30])),\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "# Write to TFRecord\n",
        "with tf.io.TFRecordWriter('data.tfrecord') as writer:\n",
        "    writer.write(create_example())\n",
        "\n",
        "# Read TFRecord\n",
        "raw_dataset = tf.data.TFRecordDataset('data.tfrecord')\n",
        "\n",
        "# Print the record\n",
        "for record in raw_dataset:\n",
        "    print(tf.train.Example.FromString(record.numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C_wIE0hA8ej_"
      },
      "outputs": [],
      "source": [
        "# Digit Recognizer in python using tensorflow datatype\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape the input data to be a 4D tensor\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Print some predictions and their corresponding labels\n",
        "for i in range(2):\n",
        "  predicted_label = np.argmax(predictions[i])\n",
        "  true_label = y_test[i]\n",
        "  print('Predicted label:', predicted_label)\n",
        "  print('True label:', true_label)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIfKz27lBoqz"
      },
      "source": [
        "[4] **Image Formats (JPG, PNG)**\n",
        "\n",
        "Usage:\n",
        "- Image storage and processing in computer vision tasks.\n",
        "\n",
        "Advantages:\n",
        "- Different compression levels, widespread compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4AM28KgBoq0"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open an image file\n",
        "img = Image.open('image.jpeg')\n",
        "\n",
        "# Convert method : Possible values :  “RGB” or “L” to “1”.\n",
        "img1 = img.convert('L')\n",
        "img2 = img.convert('1')\n",
        "\n",
        "plt.imshow(img1)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l44uEOZ-4x_w"
      },
      "outputs": [],
      "source": [
        "# prompt: Convert image into digital representation\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Open the image\n",
        "img = Image.open('image.jpeg')\n",
        "\n",
        "# Convert the image to a NumPy array\n",
        "image_array = np.array(img)\n",
        "\n",
        "# Print the complete array of the image\n",
        "#print(image_array)\n",
        "\n",
        "#You can also access specific pixel values,  to print specific entry in array\n",
        "print(image_array[2][4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VNtcTwEi5bz"
      },
      "source": [
        "**PyTorch Native Format**\n",
        "\n",
        "\n",
        " - The PyTorch native format is flexible, easy to use, and optimized for saving models during development, training, and deployment within PyTorch-based ecosystems.\n",
        "\n",
        "Use Cases :\n",
        "\n",
        "- \tSaving and loading model weights (state_dict)\n",
        "- \tSaving and loading the entire model (architecture + weights)\n",
        "- \tCheckpointing during training\n",
        "- \tModel inference and deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scOdxvGYSJ1h"
      },
      "outputs": [],
      "source": [
        "# Demo : Working of pytorch native format\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random data\n",
        "data = np.random.rand(100, 3, 224, 224)  # Example: 100 images, 3 channels, 224x224 resolution\n",
        "labels = np.random.randint(0, 10, size=(100,))  # Example: 100 labels between 0 and 9\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "data_tensor = torch.from_numpy(data).float()\n",
        "labels_tensor = torch.from_numpy(labels).long()\n",
        "\n",
        "# Print sample data\n",
        "print(\"Sample Data (First image, first channel):\")\n",
        "\n",
        "print(data_tensor[1, 1, :, :])\n",
        "print(\"Sample Label (First image):\")\n",
        "print(labels_tensor[1])\n",
        "\n",
        "plt.imshow(data_tensor[0, 0, :, :])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}